\documentclass[xcolor=dvipsnames, aspectratio=1610, 9pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % so that dollar sign does not turn into pound when italic!!
\usepackage{amsmath, amssymb, lmodern} % nice T1 compatible fonts
\usepackage[UKenglish]{babel}
\usepackage{graphicx} % for reflectbox
\setbeamertemplate{navigation symbols}{}%no nav symbols
\usetheme[secheader]{Madrid}%

\def\R{{\Large \bf R}}
\def\S{{\Large \bf S}}
\def\r{{\bf R}}
\def\s{{\bf S}}

\title{Getting to do statistics in \r}
\author[Alexandre Courtiol]{Alexandre Courtiol}
\institute[IZW]{Leibniz Institute of Zoo and Wildlife Research}%
\titlegraphic{
\vspace{0cm}
\hspace{1cm}
\includegraphics[height=2cm]{../figures/izw_logo}
\hspace{0.7cm}
\includegraphics[height=3cm]{../figures/physalia}
\hspace{1.3cm}
\includegraphics[height=2cm]{../figures/FU}
\hspace{1cm}
}
\date[June 2018]{\small June 2018}%

\begin{document}

\setlength{\topsep}{1pt}%space between input and output
<<echo = FALSE, message = FALSE>>=
options(width = 140)
library(knitr)
library(BeginR)
opts_chunk$set("size" = "scriptsize",
               "fig.width" = 5,
               "fig.height" = 5,
               "out.width" = "0.35\\linewidth",
               "out.height" = "0.35\\linewidth",
               "fig.align" = "center")
@

\AtBeginSection[]{
  \begin{frame}
  \frametitle{Getting started with \R}
  \setcounter{tocdepth}{1}
  \tableofcontents[currentsection]
  \end{frame}
}

\AtBeginSubsection[]{
  \begin{frame}
  \frametitle{Getting started with \R}
  \setcounter{tocdepth}{2}
  \tableofcontents[currentsubsection]
  \end{frame}
}

% first slide of the doc
\maketitle

\section{Some basic tests}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. the usual correlation tests:
\vfill
Just the correlation coefficient:
<<>>=
cor(x = iris$Sepal.Length, y = iris$Sepal.Width, method = "pearson")
@
\vfill
\pause
Or the actual test:
<<>>=
cor.test(x = iris$Sepal.Length, y = iris$Sepal.Width, method = "pearson")
@
\vfill
\pause
Note: two other \texttt{method}s are available: ``\texttt{spearman}'' \& ``\texttt{kendall}''.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
Note: many (not all) tests allow for the use of both standard and formula-based syntax:
\vfill
E.g.
<<>>=
cor.test(formula = ~ Sepal.Length + Sepal.Width, data = iris)
@
\vfill
is synonymous to:
<<>>=
cor.test(x = iris$Sepal.Length, y = iris$Sepal.Width)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two unpaired groups:
\vfill
The \emph{t}-test (parametric):
<<>>=
t.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
       y = iris$Sepal.Length[iris$Species == "setosa"])
@
\vfill
\pause
The Mann-Whitney U test (non-parametric):
<<>>=
wilcox.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
            y = iris$Sepal.Length[iris$Species == "setosa"])
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two paired groups:
\vfill
The paired \emph{t}-test (parametric):
<<>>=
t.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
       y = iris$Petal.Length[iris$Species == "versicolor"], paired = TRUE)
@
\vfill
\pause
The Wilcox-signed-rank test (non-parametric):
<<>>=
wilcox.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
            y = iris$Petal.Length[iris$Species == "versicolor"], paired = TRUE)
@
\vfill
\pause
Note: if you forget to specify that the data are \texttt{paired}, it won't run the right test!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing more than 2 unpaired groups:
\vfill
The Kurskal-Wallis test (non-parametric):
<<>>=
kruskal.test(formula = Petal.Length ~ Species, data = iris)
@
\vfill
\pause
The ``\emph{test for equal means in a one-way layout}'' (parametric):
<<>>=
oneway.test(formula = Petal.Length ~ Species, data = iris)
@
\vfill
\pause
Note: linear models allow for more sophisticated parametric alternatives (see later).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing more than 2 paired groups:
\vfill
The Quade test (non-parametric):
<<>>=
quade.test(y = as.matrix(iris[, c("Petal.Length", "Sepal.Length", "Petal.Width")]))
@
\vfill
\pause
The Friedman test (non-parametric):
<<>>=
friedman.test(y = as.matrix(iris[, c("Petal.Length", "Sepal.Length", "Petal.Width")]))
@
\vfill
\pause
Note: linear mixed-effects models allow for more sophisticated parametric alternatives.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing variances between groups:
\vfill
The F-test (parametric):
<<>>=
var.test(x = iris$Sepal.Length, y = iris$Petal.Length) ## max 2 groups, must be normaly distributed
@
\vfill
\pause
The Bartlett test (parametric):
<<>>=
bartlett.test(formula = Sepal.Length ~ Species, data = iris)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing variances between groups (continues):
\vfill
The Fligner test (non-parametric):
<<>>=
fligner.test(formula = Sepal.Length ~ Species, data = iris)
@
\vfill
\pause
Note: also \texttt{ansari.test()} and \texttt{mood.test()} for rank-based two-sample test for a difference in scale parameters.
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing 2 distributions:
\vfill
The Kolmogorov-Smirnov test (non-parametric):
<<>>=
ks.test(x = iris$Sepal.Length, y = iris$Petal.Length)
@
\vfill
\pause
The Shapiro-Wilk Normality test (non-parametric):
<<>>=
set.seed(2L)
shapiro.test(x = rnorm(100))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing a binomial outcome to a probability (exact):
\vfill
The exact binomial test:
<<>>=
binom.test(x = 8, n = 10, p = 0.5) ## 8 heads out of 10 coin throws -> is the coin biased?
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two independent proportions:
\vfill
The ``\emph{test of equal proportions}'':
<<>>=
prop.test(x = cbind(success = c(8, 4), failure = c(2, 6))) ## 8 heads out of 10 for one coin, 4 out of 10 for the other, do they differ?
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) of independence:
\vfill
The Fisher exact test:
<<fisher>>=
## check WorldPhones before running the code!
fisher.test(WorldPhones, simulate.p.value = TRUE, B = 100) ## simulation needed as too large for exact test!
@
\vfill
\pause
The Chi-squared test for independence:
<<>>=
chisq.test(WorldPhones)
@
\vfill
\pause
Note: the McNemar test is also available when the same subjects are measured in two conditions (see \texttt{?mcnemar.test}).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Many more simple statistical tests are available in \R \ packages}
Some examples:
\begin{itemize}
\item \texttt{coin} provides permutation implementations of many tests.
\item \texttt{nsm3} provides tons of non-parametric tests.
\item \texttt{PMCMR} provides post-hoc tests for non-parametric tests.
\item \texttt{nortest} provides several tests for normality.
\end{itemize}
\vfill
Note: this list is only a very small subset!!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{A note before we continue}
\r's original primary goal was to perform statistical analyses. So among the many thousands of
packages many focus on statistical tools and by no means I will try to cover or even
summarise this diversity.\\
\vspace{1cm}
I have chosen to only illustrate some of the tools I know and that I have used to show you how to do some statistics in \r.\\
\vspace{1cm}
Since time is limited, I will not for example illustrate any Bayesian methods, nor machine learning methods, although some good packages exist for that too!
\end{frame}


\section{Principal Component Analysis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA without packages}
PCA is a traditional method for dimentionality reduction:
<<>>=
head(USArrests)  ## original coordinates
@
\vfill
\pause
<<>>=
pca_US <- prcomp(~ Murder + Assault + Rape, data = USArrests, scale. = TRUE) ## scaling is not the default (but should be)
@
\vfill
\pause
<<>>=
head(pca_US$x)  ## new coordinates
@
\vfill
\pause
<<>>=
pca_US$rotation ## coefficients applied to original coordinate (after z-scoring them) to obtain the new coordinates by linear combination
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA without packages}
The first axis alone captures more than 78\% of the total variation in the data:
<<>>=
summary(pca_US)
@
\vfill
\pause
<<>>=
biplot(x = pca_US)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA with \texttt{ade4}}
\texttt{ade4} is a package with several multivariate tools, including the PCA:
<<>>=
library(ade4)
pca_US_ade4 <- dudi.pca(df = USArrests[, -3], scale = TRUE, scannf = FALSE)

summary(pca_US_ade4)
@
\vfill
\pause
Let us add voting data to this dataset:
<<>>=
USArrests$Vote <- rep("Trump", times = 50)
Clinton_state <- c("California", "Colorado", "Connecticut", "Delaware", "Hawaii", "Illinois", "Maine", "Maryland", "Massachusetts",
                   "Minnesota", "Nevada", "New Hampshire", "New Jersey","New Mexico", "New York", "Oregon", "Rhode Island", "Vermont",
                   "Virginia", "Washington")
USArrests[Clinton_state, "Vote"] <- "Clinton"
USArrests$Vote <- factor(USArrests$Vote)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA with \texttt{ade4}}
The package allows a different kind of plot that is quite interesting:
<<out.width="0.85\\linewidth", fig.width=12>>=
par(mfrow = c(1, 2))
s.corcircle(dfxy = pca_US_ade4$c1)
s.class(dfxy = pca_US_ade4$l1, fac = USArrests$Vote, col = c("blue", "red"))
s.label(dfxy = pca_US_ade4$l1, label = rownames(USArrests), add.plot = TRUE, clabel = 0.5)
@
\vfill
\pause
Note: using \texttt{co} \& \texttt{li} instead of \texttt{c1} \& \texttt{l1} would respect the relative contribution of each principal component according to the variance they capture.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example of application of PCA: a revision of tiger taxonomy}
\centering
\includegraphics[height = 0.2\textheight]{../figures/tiger_title}\\
\includegraphics[height = 0.7\textheight]{../figures/tiger}
\end{frame}



\section{Linear Models}
\subsection{introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What is a linear model?}

A statistical model represents, often in considerably idealized form, the data-generating process (\url{https://en.wikipedia.org/wiki/Statistical_model}).

\pause
\vfill
In a linear model, the data-generating process is assumed to be a linear function: it is constructed from a set of terms by multiplying each term by a constant (a model parameter) and adding the results.
\pause
\vfill
\r \ allows to fit efficiently and easily all main kinds of linear models:
\begin{itemize}
\item classical linear models (t-test, correlation, linear regression, ANOVA, ANCOVA): LM
\item generalized linear models (logistic regression, Poisson regression\dots): GLM
\item linear mixed-effects models: LMM
\item generalized linear mixed-effects models: GLMM
\item general additive models \& general additive mixed models: GAM \& GAMM
\end{itemize}
\vfill
\pause
Note: I have a 100 hours course on the topic (\url{https://github.com/courtiol/LM2GLMM}) but it may be a bit terse without the bla bla\dots
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Linear models in \R}
\r \ is very rich in terms of capabilities to fit linear models due to an increasing number of dedicated packages!
\vfill
For now, no other software seems to be remotely as good (prognostic: only \texttt{Julia} or \texttt{Python} may change that within a decade but I find it unlikely).
\vfill
\pause
\begin{center}
\begin{tabular}{|l|c|c|}%
\hline
Models & Packages for fitting & Helper packages\\
\hline
LM & \texttt{stats*}; \texttt{spaMM} & \texttt{car}; \texttt{lmtest}; \texttt{visreg}\\
GLM & \texttt{stats*}; \texttt{spaMM}; \texttt{pscl} & \texttt{car}; \texttt{DHARMa}; \texttt{visreg}\\
LMM & \texttt{lme4}; \texttt{spaMM}; \texttt{glmmTMB} & \texttt{DHARMa}; \texttt{pbkrtest}; \texttt{visreg}\\
GLMM & \texttt{lme4}; \texttt{spaMM}; \texttt{glmmTMB} & \texttt{DHARMa}; \texttt{pbkrtest}; \texttt{visreg}\\
GAM & \texttt{mgcv} & \texttt{DHARMa}; \texttt{visreg}\\
GAMM & \texttt{mgcv} &\\
\hline
\end{tabular}
\end{center}
\vfill
* = included in any \r \ installation!
\vfill
Note: those are my personal favorite ones, but they are plenty more out there.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Good books dealing with linear models in \R}
\begin{center}
\includegraphics[height=4cm]{../figures/Rbook}
\includegraphics[height=4cm]{../figures/PinheiroBates}
\includegraphics[height=4cm]{../figures/Gelman}
\includegraphics[height=4cm]{../figures/Zuur_MM}
\includegraphics[height=4cm]{../figures/Wood}
\end{center}
\vfill
Note: it is also useful to look at books focussed on statistics and not \r!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Preparing data for (G)LM(M)}
To maximize the chances of success prepare your data as follow:
\begin{itemize}
\item one row = one observation (if repeated measures, use several rows!)
\item qualitative variables of class \texttt{factor} (check the levels, drop unused ones, set the reference properly)
\item no \texttt{NA} (models can somewhat deal with them but it is a major source of headaches)
\item data frames (i.e. object of class \texttt{data.frame}) and not tibbles (\texttt{tbl})
\end{itemize}
\vfill
\pause
Example of a good dataset:
<<>>=
head(iris)
str(iris)
any(is.na(iris))
@
\end{frame}


\subsection{traditional linear model (LM)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{LM: notation}
Simple notation:\\
\vspace{1cm}
$y_i = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_p} \times x_{p,i} + \varepsilon_i$
\vfill
\pause
$y_i = \hat{y_i} + \varepsilon_i$
\vfill
\pause
\begin{itemize}
\item $y_i$ = the observations to explain / response variable / dependent variable
\item $\hat{y_i}$ = the fitted values
\item $x_{j,i}$ = constants derived from the predictors / explanatory variables / independent variables
\item $\hat{\beta_j}$ = the (model parameter / regression coefficient) estimates
\item $\varepsilon_i$ = the residuals (i.e. the estimates for the error which is here Gaussian with constant variance)
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{LM: notation}
Matrix notation:\\
\vspace{1cm}
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \dots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\hat{\beta_0} \\ \hat{\beta_1} \\ \hat{\beta_2} \\ \dots \\ \hat{\beta_p}
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \dots \\ \varepsilon_n
\end{bmatrix}
$$
\vfill
\pause
$Y = X \widehat{\beta} + \varepsilon = \widehat{Y} + \varepsilon$
\vfill
$\varepsilon = Y - \widehat{Y}$
\vfill
\pause
\begin{itemize}
\item $Y$ = the vector of observations
\item $\widehat{Y}$ = the vector of fitted values
\item $X$ = a matrix called the design matrix (or the model matrix)
\item $\varepsilon$ = the vector of residuals
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~ Petal.Width, data = iris)
@
\vfill
\pause
<<>>=
formula(mod)  ## the formula
@
\vfill
\pause
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
\pause
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species + Petal.Width:Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width*Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width/Species, data = iris)  ## dangerous!!!
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  1, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), , drop = FALSE]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), , drop = FALSE]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  ., data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
model_frame <- model.frame(mod)  ## the data used for the fit
model_frame[c(1:2, 51:52, 101:102), ]
@
\vfill
<<>>=
model.matrix(mod)[c(1:2, 51:52, 101:102), ]  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Understanding the design matrix}
It is most important to understand the design matrix because the interpretation of the parameters depend on it!
\vfill
The design matrix depends on:
\begin{itemize}
\item your formula
\item your data
\item the contrasts (for qualitative variables)
\end{itemize}
\vfill
\pause
By default, each category is compared to the first one:
<<>>=
mod <- lm(Petal.Length ~  Species, data = iris)
model.matrix(mod)[c(1, 51, 101), ]
@
\vfill
\pause
But other several alternative exist; e.g.:
<<>>=
mod2 <- lm(Petal.Length ~  Species, data = iris, contrasts = list(Species = "contr.sum"))
model.matrix(mod2)[c(1, 51, 101), ]
@
\vfill
\pause
Note 1: default contrats (\texttt{"contr.treatment"}) are easy to interpret!
\vfill
\pause
Note 2: contrasts do not alter predicted values and thus likelihood, AIC\dots
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Understanding the design matrix}
Challenge: find out whether these different representations of gender are equivalent or not?
\vfill
\begin{itemize}
\item \texttt{"boy"} vs \texttt{"girl"}
\item \texttt{"male"} vs \texttt{"female"}
\item \texttt{0} vs \texttt{1}
\item \texttt{1} vs \texttt{2}
\item \texttt{TRUE} vs \texttt{FALSE}
\end{itemize}
\vfill
Note: no need to fit a model, you can use the function \texttt{model.matrix()} with a formula!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM outputs: parameter estimates}
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
Simply printing the object provides you with the parameter estimates:
<<>>=
mod
@
\vfill
\pause
If you need to work with them, use the specific extractor instead:
<<>>=
coefficients(mod) ## or coef(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM outputs: parameter estimates}
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
You can also easily extract the covariance matrix of the estimates:
<<>>=
vcov(mod)
@
\vfill
\pause
And thus the standard errors:
<<>>=
sqrt(diag(vcov(mod)))
@
\vfill
\pause
You can also get confidence intervals:
<<>>=
confint(mod)
@
\vfill
\pause
Note: that reveals that there are much more information in the object \texttt{mod} than it is being printed!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM outputs: the model object}
The fitted model object is in fact a big list of class \texttt{"lm"}:
<<>>=
class(mod)
typeof(mod)
names(mod)
@
\vfill
So you can extract information from it; e.g.:
<<>>=
mod$df.residual
@
but it is safer to use extractors if they are available!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM: example of other outputs}
There are quite a few extractors out there:
<<>>=
logLik(mod)
AIC(mod)
@
\vfill
\pause
Here is how you can get the list of \texttt{S3} methods for the class \texttt{"lm"}:
<<>>=
methods(class = "lm")
@
\vfill
Note: the list will change depending on the packages that are attached to the \r \ session!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: coefficients}
For LM, simply use \texttt{summary()}:
<<>>=
summary(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: predictors}
Don't use the default \texttt{anova()} function which performs type-I analysis-of-variance:
<<>>=
anova(mod)
@
\vfill
Instead, better use the function \texttt{Anova()} from the package \texttt{car} which performs type-II analysis-of-variance:
<<message = FALSE>>=
library(car)
Anova(mod)
@
\vfill
\pause
Note: p-values are the same no matter the order of the predictors in the formula for type-II (but not for type-I!).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: the overall model}
Before looking at significance for estimates or predictor, always start by checking that your model fits the data better than a null model:
\vfill
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
mod_null <-  lm(Petal.Length ~  1, data = iris)
anova(mod, mod_null)
@
\vfill
Note 1: this was also given at the bottom of the summary table!\\
\vspace{1em}
Note 2: here using \texttt{anova()} is perfectly fine!
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: fitted values}
You can easily obtain the prediction for your observation (i.e. fitted values):
<<>>=
fitted(mod)[1:39]
@
\vfill
\pause
As expected, observations are equal to the fitted values + residuals:
<<>>=
head(cbind("response" = model.response(model.frame(mod)),
           "fitted" = fitted(mod),
           "resid" = residuals(mod),
           "fitted + resid" = fitted(mod) + residuals(mod)))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: fast and dirty plot}
<<"fig.width" = 10, "out.width" = "0.7\\linewidth">>=
library(visreg)
par(mfrow = c(1, 2))
visreg(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM predictions: fast \& less dirty plot}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
library(visreg)
library(ggplot2)
visreg(fit = mod, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE) +
  theme_classic()
@
\vfill
Note: if you have different quantitative predictors you can specify the value for the non focal predictor using the argument \texttt{"cond"}.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: by ``hand''}
The most difficult step is to create the data frame defining the predictor values:
<<message = FALSE>>=
library(dplyr)
data_for_predictions <- iris %>%
                            group_by(Species) %>%
                            do(data.frame(Petal.Width = seq(min(.$Petal.Width), max(.$Petal.Width), length.out = 30))) %>%
                            data.frame()
@
\begin{columns}
\column{0.4\linewidth}
<<>>=
head(data_for_predictions)
@
\column{0.4\linewidth}
<<>>=
tail(data_for_predictions)
@
\end{columns}
\vfill
\pause
Then, it is easy:
<<>>=
pred_mod <- predict(object = mod, newdata = data_for_predictions, interval = "confidence") ## prediction intervals are also possible!
head(pred_mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: by ``hand''}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
data_for_plot <- cbind(pred_mod, data_for_predictions)
ggplot(data = data_for_plot, mapping = aes(x = Petal.Width, y = fit, colour = Species)) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = lwr, ymax = upr, fill = Species), alpha = 0.2) +
  geom_point(data = iris, mapping = aes(y = Petal.Length, x = Petal.Width, colour = Species)) +
  labs(x = "Petal Width", y = "Petal Length") +
  theme_classic()
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM assumptions: generalities}
Model structure:
\begin{itemize}
\item linearity
\item lack of perfect multicollinearity (design matrix of full rank)
\item predictor variables have fixed values
\end{itemize}
\pause
\vfill
Errors:
\begin{itemize}
\item independence (no serial autocorrelation)
\item constant variance (homoscedasticity)
\item normality
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: linearity}
Departure from linearity can originate from a multitude of reasons and can create all kinds of problems.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item thinking
\item other assumptions violated
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item different model structure $\rightarrow$ change the formula
\item transform one or several predictors (e.g. polynomials) $\rightarrow$ function \texttt{poly()}
\item transform the response (e.g. log and power transformation) $\rightarrow$ function \texttt{powerTransform()} in \texttt{car}\\ (see later)
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item non-linear models $\rightarrow$ function \texttt{nls} or dedicated package (e.g. \texttt{nlme})
\item general additive models $\rightarrow$ package \texttt{mgcv}
\end{itemize}
\vfill
\pause
Quiz: can you express the following models as LM?
\begin{itemize}
\item $y_i = \hat{\alpha} + \varepsilon_i$
\item $y_i = x_i^{\hat{\beta}} + \varepsilon_i$
\item $y_i = \hat{\alpha} + \hat{\beta_1} x_i + \hat{\beta_2} x_i^2 + \hat{\beta_3} x_i^3 + \varepsilon_i$
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: lack of perfect multicollinearity}
The number of parameters to be estimated must be equal to the rank of the design matrix.
\vfill
Caused by having less data than parameters or when there is linear dependence between the column vectors of the design matrix. In such case, some parameters cannot be computed.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item plot the predictors against each other $\rightarrow$ function \texttt{pairs()}
\item \texttt{findLinearCombos} from the package \texttt{caret}
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item change design matrix (change parameterization or drop redundant effects) $\rightarrow$ argument \texttt{formula}
\item change the experimental design
\item collect more data
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item none
\end{itemize}
\vfill
\pause
Note: strong albeit imperfect collinearity is not great either; check correlation between estimates ($\rightarrow$ \texttt{cov2cor(vcov(mod))}) and variance inflation factors ($\rightarrow$ \texttt{vif(mod)}).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: lack of perfect multicollinearity}
Example of perfect multicollinearity:
<<>>=
iris_silly <- iris
iris_silly$Petal.Surface <-  iris_silly$Petal.Length*iris_silly$Petal.Width
mod_silly <- lm(Sepal.Width ~ log(Petal.Length) + log(Petal.Width) + log(Petal.Surface), data = iris_silly)
summary(mod_silly)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: predictor variables have fixed values}
The dependent variable are represented by fixed values.
\vfill
The presence of measurement errors is the main cause of violation. Violation can trigger both estimates and tests to be biased.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item thinking \& replication
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item often ignored in practice
\item better measurements
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item multipurpose numerical approaches $\rightarrow$ function \texttt{optim()} or dedicated packages (e.g. \texttt{nloptr}, \texttt{rjags}, \texttt{nimble}, \texttt{rstan})
\item errors-in-variables models $\rightarrow$ not much directly but any procedure allowing for latent variables can handle that; packages (e.g. \texttt{sem}, \texttt{lavaan}, \texttt{OpenMX})
\item reduced major axis regression $\rightarrow$ dedicated packages (e.g. \texttt{lmodel2})
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: independence (no serial autocorrelation)}
A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). Lack of independence increases the risk of false positive (sometimes a lot).\\
\vspace{1em}
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 1)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: independence (no serial autocorrelation)}
A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). Lack of independence increases the risk of false positive (sometimes a lot).\\
\vspace{1em}
Diagnostic by Durbin-Watson test:
<<>>=
durbinWatsonTest(mod) ## from package car (DW varies between 0 & 4, 2 is best, you wish for non-significant p-value)
@
Note: the alternative from the package \texttt{lmtest} offer to rank the residuals according to a variable.
\vfill
\pause
Solutions:
\begin{itemize}
\item transformation or different model structure (see linearity)
\item aggregation or sub-sampling
\end{itemize}
\pause
\vfill
Alternatives:
\begin{itemize}
\item general additive models (GAM and GAMM) $\rightarrow$ dedicated package \texttt{mgcv}
\item mixed models (LMM and GLMM) $\rightarrow$ dedicated packages (e.g. \texttt{spaMM}, \texttt{lme4})
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: constant variance (homoscedasticity)}
Heteros(c/k)edasticity can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. It can create both false positives and false negative.
\vfill
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 3)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: constant variance (homoscedasticity)}
Heteros(c/k)edasticity can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. It can create both false positives and false negative.
\vfill
Diagnostic by Breusch-Pagan test:
<<message = FALSE>>=
library(lmtest)
bptest(mod)  ## BP = df is best, you wish for non-significant p-value
@
\vfill
\pause
Solutions: modeling the heteroscedasticity
<<message = FALSE>>=
library(spaMM)
mod_heter_spaMM <- fitme(Petal.Length ~  Petal.Width + Species,
                         resid.model = ~ Species,
                         data = iris)
AIC(mod)
print(AIC(mod_heter_spaMM)) ## much better fit!
@
\pause
\vfill
Alternatives:
\begin{itemize}
\item GLM (if stemming from an expected relationship between mean and variance) $\rightarrow$ function \texttt{glm}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: normality}
The distribution of residuals can be skewed, this is often caused by the presence of outliers, and/or when the process generating the data is very different from normal (e.g. Poisson, Binomial...).
\vfill
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 2)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: normality}
The distribution of residuals can be skewed, this is often caused by the presence of outliers, and/or when the process generating the data is very different from normal (e.g. Poisson, Binomial...).
\vfill
Diagnostic by test (many test are possible):
<<>>=
shapiro.test(mod$residuals)  ## stat = 1 when normal, you wish for non-significant p-value
@
\vfill
\pause
Solutions:
\begin{itemize}
\item transformation or different model structure (see linearity)
\item taking outliers out (mindfully!)
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item GLM (if stemming from the data generating process) $\rightarrow$ function \texttt{glm}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: simple glimpse}
You can check all assumptions about the erros at once:
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.45\\linewidth", "out.height" = "0.45\\linewidth">>=
par(mfrow = c(2, 2))
plot(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: outliers}
There is a powerful function in \r:
<<>>=
influence.measures(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM assumptions: outliers}
Interpretation of the output from \texttt{influence.measures(mod)}:
\vfill
\begin{small}
\begin{itemize}
\item<1-> \texttt{dfb.1\_} $\rightarrow$ extent to which the intercept changes if a given observation is dropped
\item<1-> \texttt{dfb.Pt.W} $\rightarrow$ extent to which the slope for \texttt{Petal.Width} changes if a given observation is dropped
\item<1-> \texttt{dfb.Spcsvrs} $\rightarrow$ extent to which the estimate for \texttt{versicolor} changes if a given observation is dropped
\item<1-> \texttt{dfb.Spcsvrg} $\rightarrow$ extent to which the estimate for \texttt{virginica} changes if a given observation is dropped
\item<2-> \texttt{dffit} $\rightarrow$ extent to which the predicted y-values changes if a given observation is dropped (scaled by the standard deviation of the fit at the point)
\item<3-> \texttt{cov.r} $\rightarrow$ extent to which the covariance matrix of parameter estimates changes if a given observation is dropped
\item<4-> \texttt{cook.d} $\rightarrow$ $F$ statistics comparing simultaneously the changes in all estimates when the observation is dropped or not
\item<5-> \texttt{hat} $\rightarrow$ diagonal element of the hat matrix (the hat values); extent to which an observation is unusual in terms of X values (leverage)
\item<6-> \texttt{inf} $\rightarrow$ some overall add hoc recipe to spot influential observations (not to be taken too seriously)
\end{itemize}
\end{small}
\vfill
Note: for the df-betas, the name would change for another model as they use the abbreviated name of the estimates:
<<>>=
abbreviate(stats:::variable.names.lm(mod))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: outliers}
There are also plotting possibilitites:
<<"fig.width" = 12, "out.width" = "0.9\\linewidth">>=
par(mfrow = c(1, 3))
plot(mod, which = 4:6)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: simple glimpse at residuals}
What would it look like if it was perfect?
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.4\\linewidth", "out.height" = "0.4\\linewidth">>=
iris$Fake.Petal.Length <- simulate(object = mod)[, 1]  ## if you rerun that, it will change each time!
mod_perfect <- lm(Fake.Petal.Length ~  Petal.Width + Species, data = iris)
par(mfrow = c(2, 2))
plot(mod_perfect)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Fixing attempt:
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.4\\linewidth", "out.height" = "0.4\\linewidth">>=
bc <- powerTransform(mod)
iris$Petal.Length_bc <- bcPower(iris$Petal.Length, lambda = bc$lambda)
mod_bc <- lm(Petal.Length_bc ~ Petal.Width + Species, data = iris)
par(mfrow = c(2, 2))
plot(mod_bc)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Plotting predictions:
\vfill
<<"fig.width" = 7, out.width = "0.5\\linewidth">>=
visreg(fit = mod_bc, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE) +
  theme_classic()
@
\vfill
Note: that is not very useful because it is on the BoxCoxed scale!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Plotting predictions:
\vfill
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
visreg(fit = mod_bc, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE,
       trans = function(x) bcnPowerInverse(x, lambda = bc$lambda, gamma = 0), partial = TRUE) +
  theme_classic()
@
\end{frame}


\subsection{generalised linear models (GLM)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{GLM: what for?}
GLM are used for fitting data generating processes for which a relationship between mean and variance is expected.
\vfill
\pause
That includes the analysis of:
\vspace{1em}
\begin{itemize}
\item binary events (probabilities)
\vspace{1em}
\item binomial events (probabilities)
\vspace{1em}
\item Poisson processes (counts)
\vspace{1em}
\item negative binomial processes (counts)
\vspace{1em}
\item variances (positive continuous)
\end{itemize}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{GLM: notation}
Definition:\\
\vspace{1em}
$\text{Y} = g^{-1}(\widehat{\eta})+ \varepsilon = g^{-1}(\text{X}\widehat{\beta}) + \varepsilon$
\vfill
with:
\begin{itemize}
\item $\hat{\eta_i} = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_{p}} \times x_{p,i}$
\item $\text{E}(\text{Y}) = \mu = g^{-1}(\eta)$
\item $\text{Var}(\text{Y}) = \phi\text{V}(\mu)$
\end{itemize}
\vfill
Notation:
\begin{itemize}
\item $\eta$ the linear predictor
\item $g$ the link function ($g^{-1}$ is sometimes called the mean function)
\item $\text{V}$ the variance function
\item $\phi$ is the dispersion parameter
\end{itemize}
\vfill
\pause
This is identical to the LM if:
\begin{itemize}
\item $\mu = g^{-1}(\eta) = \eta$, thus if $g$ is the identity function
\item $\phi = \sigma^2$, thus if the dispersion parameter equals the error variance
\item $\text{V}(\mu) = 1$, thus if the variance function is constant
\end{itemize}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The \texttt{Challenger} dataset}
\begin{center}
\includegraphics[height = 5cm]{../figures/Challenger}
\end{center}
<<>>=
head(Challenger, n = 3L)
@
\vfill
Note: we will study both the probability that one O-ring fails (binary event) or that at least one O-ring fails (binomial event) as a function of the temperature and the leak-check pressure.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The \texttt{VonBort} dataset}
\begin{center}
\includegraphics[height = 5cm]{../figures/prussians}
\end{center}
<<>>=
head(VonBort, n = 3L)
@
\vfill
Note: we will compare the number of deaths caused by horse (or mule) kicks between the 14 corps of the Prussian army.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM: specifications}
In \r \ notation:\\
\vspace{1em}
<<>>=
Challenger$issue <- Challenger$oring_dt > 0

mod_challenger_binar <- glm(issue ~ temp + psi, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
\pause
<<>>=
Challenger$oring_ok <- Challenger$oring_tot - Challenger$oring_dt

mod_challenger_binom <- glm(cbind(oring_dt, oring_ok) ~ temp + psi, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
\pause
<<>>=
mod_horsekick <- glm(deaths ~ corps, family = poisson(link = "log"), data = VonBort)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: specifications}
The \texttt{family} object contains all kinds of useful information required for the fitting procedure:
<<>>=
names(binomial(link = "logit"))
@
\vfill
\pause
The link function:
<<>>=
probs <- seq(0.1, 0.9, by = 0.1)
probs
logits <- binomial(link = "logit")$linkfun(mu = probs)
logits
@
\vfill
\pause
The inverse link function:
<<>>=
binomial(link = "logit")$linkinv(eta = logits)
@
\vfill
\pause
The variance function:
<<>>=
binomial(link = "logit")$variance(mu = probs)
@
\vfill
\pause
Note: you can use these functions to better understand GLM or when you need them to process some outputs.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: outputs}
<<>>=
mod_challenger_binar
confint(mod_challenger_binar)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: outputs}
<<>>=
mod_challenger_binom
confint(mod_challenger_binom)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: outputs}
<<>>=
mod_horsekick
head(confint(mod_horsekick))
@
\vfill
\pause
Note: \alert{but the interpretation of the parameters is very different since they are expressed on the scale of the linear predictor!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM outputs: parameter estimates}
There is no general receipe, it all depends on the link function used\dots
\pause
\begin{itemize}
\item For logistic regressions (\texttt{link = "logit"}; not for all binomial models), use odd-ratios:
\end{itemize}
<<>>=
exp(coef(mod_challenger_binar)["temp"])
1/exp(coef(mod_challenger_binar)["temp"])
@
Every decrease by one degree increases the odd of failure for at least one O-ring by \Sexpr{round(1/exp(coef(mod_challenger_binar)["temp"]), 1)} time!
\vfill
<<>>=
exp(coef(mod_challenger_binom)["temp"])
1/exp(coef(mod_challenger_binom)["temp"])
@
Every decrease by one degree increases the odd of failure for exactly one O-ring by \Sexpr{round(1/exp(coef(mod_challenger_binom)["temp"]), 1)} time!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM outputs: parameter estimates}
There is no general receipe, it all depends on the link function used\dots
\begin{itemize}
\item For logistic regressions (\texttt{link = "logit"}; not for all binomial models), use odd-ratios.
\item For Poisson regressions (\texttt{link = "log"}; not for all binomial models), use proportional increase:
\vfill
<<>>=
exp(coef(mod_horsekick)["corpsXIV"])
@
The army corp XIV received 1.5 times more kicks than the corp G (reference).
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM tests: the overall model}
<<>>=
mod_challenger_binar_null <- glm(issue ~ 1, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
<<>>=
anova(mod_challenger_binar, mod_challenger_binar_null, test = "LRT")
@
\vfill
\pause
<<>>=
mod_challenger_binom_null <- glm(cbind(oring_dt, oring_ok) ~ 1, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
<<>>=
anova(mod_challenger_binom, mod_challenger_binom_null, test = "LRT")
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM tests: the overall model}
<<>>=
mod_horsekick_null <- glm(deaths ~ 1, family = poisson(link = "log"), data = VonBort)
@
\vfill
<<>>=
anova(mod_horsekick, mod_horsekick_null, test = "LRT")
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM tests: predictors}
<<>>=
Anova(mod_challenger_binar)
@
<<>>=
Anova(mod_challenger_binom)
@
<<>>=
Anova(mod_horsekick)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM tests: coefficients}
<<>>=
summary(mod_challenger_binar)
@
\vfill
\pause
Note: these z-tests are asymptotic tests relying on normality of parameter estimates. They are not to be trusted with small dataset for GLM! There is no easy alternative, so better trust the likelihood ratio test of the overall predictor!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM tests: coefficients}
<<>>=
summary(mod_challenger_binom)
@
\vfill
Note: these z-tests are asymptotic tests relying on normality of parameter estimates. They are not to be trusted with small dataset for GLM! There is no easy alternative, so better trust the likelihood ratio test of the overall predictor!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM tests: coefficients}
<<>>=
summary(mod_horsekick)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
Fitted values are on the scale of probabilities:
<<>>=
fitted(mod_challenger_binar)
@
\vfill
\pause
<<>>=
predict(mod_challenger_binar, type = "link") ## the default!!!!!!!!
@
<<>>=
predict(mod_challenger_binar, type = "response") ## the useful one!!!!!!!!!
@
<<>>=
predict(mod_challenger_binar, newdata = data.frame(temp = 31, psi = mean(Challenger$psi)), type = "response")
@
\vfill
\pause
Note: the crash was almost inevitable (but we extrapolate)!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
Fitted values are on the scale of probabilities:
<<>>=
fitted(mod_challenger_binom)
@
\vfill
\pause
<<>>=
predict(mod_challenger_binom, type = "link") ## the default!!!!!!!!
@
<<>>=
predict(mod_challenger_binom, type = "response") ## the useful one!!!!!!!!!
@
<<>>=
predict(mod_challenger_binom, newdata = data.frame(temp = 31, psi = mean(Challenger$psi)), type = "response")
@
\vfill
Note: the crash was almost inevitable (but we extrapolate)!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
Fitted values are on the scale of counts:
<<>>=
fitted(mod_horsekick)[1:30]
@
\vfill
<<>>=
predict(mod_horsekick, type = "link")[1:30] ## the default!!!!!!!!
@
\vfill
<<>>=
predict(mod_horsekick, type = "response")[1:30] ## the useful one!!!!!!!!!
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM: predictions}
Try that:
<<eval = FALSE>>=
visreg2d(mod_challenger_binar, xvar = "temp", yvar = "psi", plot.type = "rgl", scale = "response") ## I cannot do that easily inside a PDF...
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<"fig.width" = 7, "out.width" = "0.55\\linewidth">>=
visreg2d(mod_challenger_binar, xvar = "temp", yvar = "psi", plot.type = "persp", scale = "response")
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<"fig.width" = 8, "out.width" = "0.55\\linewidth">>=
visreg2d(mod_challenger_binar, xvar = "temp", yvar = "psi", plot.type = "image", scale = "response")
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<"fig.width" = 8, "out.width" = "0.55\\linewidth">>=
visreg2d(mod_challenger_binar, xvar = "temp", yvar = "psi", plot.type = "gg", scale = "response")
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<>>=
visreg(fit = mod_challenger_binar, xvar = "temp",
       cond = list(psi = mean(Challenger$psi)),
       gg = TRUE, scale = "response") +
  geom_vline(xintercept = 31, colour = "red", lty = 2) +
  theme_classic()
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<>>=
visreg(fit = mod_challenger_binom, xvar = "temp",
       cond = list(psi = mean(Challenger$psi)),
       gg = TRUE, scale = "response") +
  geom_vline(xintercept = 31, colour = "red", lty = 2) +
  theme_classic()
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: predictions}
<<>>=
visreg(fit = mod_horsekick, gg = TRUE, scale = "response") +
  theme_classic() + ylim(min = 0, max = 2)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: generalities}
Model structure:
\begin{itemize}
\item linearity
\item lack of perfect multicollinearity (design matrix of full rank)
\item predictor variables have fixed values
\end{itemize}
\pause
\vfill
Errors:
\begin{itemize}
\item independence (no serial autocorrelation)
\item lack of overdispersion and underdispersion
\end{itemize}
\vfill
Other:
\begin{itemize}
\item no data separation or quasi-separation
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
Very difficult to diagnose...
\vfill
You may try to:
\begin{itemize}
\item plot partial residuals using \texttt{car}
\item compare prediction to GAM using \texttt{mgcv}
\item overall goodness of fit test using \texttt{DHARMa}
\end{itemize}
\vfill
\pause
Note: same fix as for LM!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
Partial residual plots using \texttt{car}:
\vfill
<<"fig.width" = 8, "out.width" = "0.7\\linewidth">>=
crPlots(mod_challenger_binar)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
General Additive Model fit using \texttt{mgcv}:
\vfill
<<message = FALSE>>=
library(mgcv)
mod_challenger_binar_GAM <- gam(issue ~ s(temp) + psi, family = binomial(link = "logit"), data = Challenger)
plot(mod_challenger_binar_GAM, shade = TRUE, shade.col = "red")  ## on the scale of the linear predictor!
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
library(DHARMa)  ## package to create USEFUL residuals in GLM, GAM and GLMM
res_mod_challenger_binar <- simulateResiduals(mod_challenger_binar)
plot(res_mod_challenger_binar)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
res_mod_challenger_binom <- simulateResiduals(mod_challenger_binom)
plot(res_mod_challenger_binom)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: linearity}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
res_mod_horsekick <- simulateResiduals(mod_horsekick)
plot(res_mod_horsekick)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: multicollinearity \& fixed-values}
\begin{center}
\large{Same as for LM!}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: lack for serial autocorrelation}
<<>>=
testTemporalAutocorrelation(res_mod_challenger_binar, time = Challenger$temp)
@
\vfill
Note: this is best done for all predictors and according to predicted values!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: correct amount of dispersion}
A GLM assumes a particular relationship between mean and variance:
<<"fig.width" = 10, "out.width" = "0.8\\linewidth">>=
p <- seq(0, 1, 0.1)
lambda <- 0:10
theta <- 0:10
v_b <- binomial()$variance(p)
v_p <- poisson()$variance(lambda)
v_G <- Gamma()$variance(theta)
par(mfrow = c(1, 3), las = 2)
plot(v_b ~ p); plot(v_p ~ lambda); plot(v_G ~ theta)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: correct amount of dispersion}
\begin{itemize}
\item Overdispersion = more variance than expected
\begin{itemize}
\item very common $\rightarrow$ increases false positive
\vfill
\item specially relevant for Poisson and Binomial
\vfill
\item irrelevant for the binary case! (don't look for it)
\vfill
\item Usual suspects:
\begin{itemize}
\item lack of linearity
\item unobserved heterogeneity
\item zero-augmentation
\end{itemize}
\end{itemize}
\vfill
\item Underdispersion = less variance than expected
\begin{itemize}
\item rather rare $\rightarrow$ increases false negative
\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM assumptions: testing overdispersion}
Usual recommended tests are quite poor (deviance / residual degree of freedom\dots).
\vfill
A much better alternative: use \texttt{DHARMa}:
<<>>=
testDispersion(res_mod_challenger_binom)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM assumptions: testing overdispersion}
Usual recommended tests are quite poor (deviance / residual degree of freedom\dots).
\vfill
A much better alternative: use \texttt{DHARMa}:
<<>>=
testDispersion(res_mod_horsekick)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: fixing overdispersion}
Potential solutions:
\begin{itemize}
\item fix linearity issues
\vfill
\item fix heterogeneity issues (if you have the data)
\vfill
\item model the overdispersion (e.g. using \texttt{spaMM})
\vfill
\item try another probability distribution
\begin{itemize}
\item Poisson overdispersed? $\rightarrow$ quasipoisson overdispersed? $\rightarrow$ Negative binomial or COMPoisson (e.g. using \texttt{spaMM})
\item Binomial overdispersed? $\rightarrow$ quasibinomial or GLMM with random effect per observation
\end{itemize}
\vfill
\item there are specific solutions if the origin is zero-augmentation!!
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: the right amount of zeros}
Do we have too many zeros?
<<>>=
testZeroInflation(res_mod_horsekick)  ## again from DHARMa
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: the right amount of zeros}
Why could you have too many zeros (in GLM)?
\vfill
It can occur when the response results from a 2 (or more) steps process
\vfill
Examples:
\begin{itemize}
\item detection issue (low counts are less detected, e.g. counting cells on microscope)
\item biological (e.g. infection, then spread of microbes)
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{GLM assumptions: the right amount of zeros}
We have 2 main options if you have too many zeros:
\vfill
\begin{itemize}
\item Fit an hurdle model:
\begin{itemize}
\item binomial (or truncated count distribution) + truncated Poisson or truncated negative binomial
\item a single source of zeros (e.g. measuring device fails)
\end{itemize}
\vfill
\item Fit a zero-inflation model:
\begin{itemize}
\item binomial (or truncated count distribution) + Poisson or negative binomial
\item two sources of zeros (e.g. number of viruses in individuals 0 for unexposed, 0 for exposed with strong immune system)
\end{itemize}
\end{itemize}
\vfill
\pause
Note 1: If you are not sure where the zeros come from, try both!
\vfill
Note 2: both solutions are implemented in the package \texttt{pscl} for GLM (and \texttt{glmmTMB} for mixed models).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Example of zero-inflation}
<<message = FALSE>>=
library(pscl)
data("bioChemists", package = "pscl")
head(bioChemists)
@
\vfill
<<>>=
mod_bioch_poiss <- glm(art ~ ., data = bioChemists, family = poisson(link = "log"))
@
\vfill
<<>>=
res_mod_bioch_poiss <- simulateResiduals(mod_bioch_poiss)
testOverdispersion(res_mod_bioch_poiss, plot = FALSE)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Example of zero-inflation}
<<>>=
testZeroInflation(res_mod_bioch_poiss)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Example of zero-inflation}
<<>>=
mod_bioch_zinfl <- zeroinfl(art ~ . | 1, data = bioChemists, dist = "poisson")
mod_bioch_zinfl  ## mind that binomial model measures excess of _non-zero_
@
\vfill
<<>>=
mod_bioch_hurdle <- hurdle(art ~ . | 1, data = bioChemists, dist = "poisson")
mod_bioch_hurdle  ## mind that binomial model measures excess of _zero_
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Example of zero-inflation}
<<>>=
mod_bioch_zinfl2 <- zeroinfl(art ~ . | 1, data = bioChemists, dist = "negbin")
mod_bioch_zinfl2  ## mind that binomial model measures excess of _non-zero_
@
\vfill
<<>>=
mod_bioch_hurdle2 <- hurdle(art ~ . | 1, data = bioChemists, dist = "negbin")
mod_bioch_hurdle2  ## mind that binomial model measures excess of _zero_
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Example of zero-inflation}
<<>>=
library(lmtest)
lrtest(mod_bioch_zinfl, mod_bioch_zinfl2, mod_bioch_hurdle, mod_bioch_hurdle2)
@
\vfill
Note: the model \texttt{mod\_bioch\_zinfl2} is clearly the best one.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Example of zero-inflation}
You can then evaluate then as usual:
<<eval = FALSE>>=
lrtest(mod_bioch_zinfl2) ## test against null model
Anova(mod_bioch_zinfl2)  ## test predictors
summary(mod_bioch_zinfl2) ## test parameter estimates (approximative)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM assumptions: no separation}
Separation occurs when a level or combination of levels for categorical predictor, or when a particular threshold along a continuous predictor, predicts the outcomes perfectly. It prevents the model to fit or lead to silly estimates and SE.
\vfill
<<>>=
set.seed(1L)
n <- 50
myxococcus <- data.frame(sporulation = rbinom(2*n, prob = c(rep(0, n), rep(0.75, n)), size = 1),
                   strain = factor(c(rep("sp1", n), rep("sp2", n))))
table(myxococcus$sporulation, myxococcus$strain)
@
\vfill
\pause
<<>>=
mod <- glm(sporulation ~ strain, data = myxococcus, family = binomial(link = "log"))
summary(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM assumptions: fixing separation}
One possible way out is to use the package \texttt{brglm2}
<<>>=
library(brglm2)
mod2 <- glm(sporulation ~ strain, data = myxococcus, family = binomial(link = "log"), method = "brglmFit")
summary(mod2)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{GLM: challenge}
Try to understand what influenced survival (i.e. access to lifeboats) during the Titanic disaster.
\begin{center}
\includegraphics[height = 5cm]{../figures/Titanic}
\end{center}
<<>>=
head(TitanicSurvival)
@
\end{frame}


\subsection{mixed models (LMM \& GLMM)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: introduction}
Liner mixed-effects models (LMM) and generalised linear mixed-effects models (GLMM) are extensions from, respectivelly, LM and GLM for situations for which there is covariation between the errors.
\vfill
\pause
Mixed-effects models allow for:
\begin{itemize}
\item the study of other questions than LM (e.g. heritability)
\item the fixing of assumption violations in LM (lack of dependence, some cases of overdispersion)
\item the reduction of the uncertainty in estimates and predictions in cases where many parameters would have to be estimated at the cost of an additional hypothesis (the distribution of the random effects)
\end{itemize}
\vfill
\pause
The main sources of heterogeneity considered by mixed-effects models are:
\begin{itemize}
\item origin (in its widest sense)
\item time
\item space
\end{itemize}
\vfill
\pause
Note 1: how to perform predicitons, tests, checks of the assumptions\dots properly requires to first master (G)LM and is matter of current research; so it is beyond the context of this short introduction.
\vfill
Note 2: for good packages, check \texttt{lme4}, \texttt{spaMM} \& \texttt{glmmTMB}.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can account for replicates!
\vfill
Example:
<<>>=
data("Oats", package = "nlme")
#coplot(yield ~ nitro | Variety + Block, data = Oats, type = "b")
library(spaMM)
mod_yield_spaMM <- fitme(yield ~ nitro + Variety + (1|Block), data = Oats)
mod_yield_spaMM
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can account for spatial and/or temporal autocorrelation!
\vfill
Example:
<<loaloa, "fig.width" = 10, "out.width" = "0.7\\linewidth">>=
data("Loaloa", package = "spaMM")
ndvi <- Loaloa[, c("maxNDVI", "latitude", "longitude")]
mod_ndvi_spaMM <- fitme(maxNDVI ~ 1 + Matern(1|longitude + latitude), data = ndvi, method = "REML")
filled.mapMM(mod_ndvi_spaMM, add.map = TRUE, plot.title = title(xlab = "Longitude", ylab = "Latitude"))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can estimate the effects of variable(s) that you cannot measure!
\vfill
Example: the estimation of genetic additive effects and heritability.
\begin{center}
\includegraphics[height = 0.2\textheight]{../figures/AnimalModel}
\reflectbox{\includegraphics[height = 0.55\textheight]{../figures/Gryphon}}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can estimate the effects of variable(s) that you cannot measure!
\vfill
Example: the estimation of genetic additive effects and heritability.
<<>>=
tail(Gryphon$pedigree)
@
\vfill
\pause
<<>>=
library(nadiv)
A <- as(makeA(Gryphon$pedigree), "matrix")
colnames(A) <- rownames(A) <- Gryphon$pedigree$ID
A[1305:1309, 1296:1309]
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can estimate the effects of variable(s) that you cannot measure!
\vfill
Example: the estimation of genetic additive effects and heritability.
<<>>=
mod_gryphon <- fitme(BWT ~ 1 + corrMatrix(1|ID), corrMatrix = A, data = Gryphon$data, method = "REML")
mod_gryphon
@
\vfill
\pause
<<>>=
(h2 <- as.numeric(mod_gryphon$lambda / (mod_gryphon$lambda + mod_gryphon$phi)))
@
\vfill
Note: the model is a little slow to fit ($\sim$60 sec, but optimisations are possible)%we can use sparse matrix but with HLCor and precision matrix... I prefer not to go there.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can estimate the effects of variable(s) that you cannot measure!
\vfill
Example: the estimation of genetic additive effects and heritability.
<<>>=
curve(dnorm(x, sd = sqrt(as.numeric(mod_gryphon$lambda["ID"]))), from = -3, to = 3, las = 1,
      ylab = "pdf", xlab = "predicted breeding value")
BLUPs <- ranef(mod_gryphon)$"corrMatrix(1 | ID)"
points(dnorm(BLUPs, sd = sqrt(as.numeric(mod_gryphon$lambda["ID"]))) ~ BLUPs, col = "blue", type = "h", lwd = 0.1)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{(G)LMM: showcase}
With mixed models, you can estimate the effects of variable(s) that you cannot measure!
\vfill
Example: the estimation of genetic additive effects and heritability.
<<>>=
plot(BLUPs ~ scale(mod_gryphon$data$BWT), ylab = "Predicted breeding values", xlab = "Scaled phenotypic values")
@
\end{frame}


\end{document}
