\documentclass[xcolor=dvipsnames, aspectratio=1610, 9pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % so that dollar sign does not turn into pound when italic!!
\usepackage{amsmath, amssymb, lmodern} % nice T1 compatible fonts
\usepackage[UKenglish]{babel}
\setbeamertemplate{navigation symbols}{}%no nav symbols
\usetheme[secheader]{Madrid}%

\def\R{{\Large \bf R}}
\def\S{{\Large \bf S}}
\def\r{{\bf R}}
\def\s{{\bf S}}

\title{Getting to do statistics in \r}
\author[Alexandre Courtiol]{Alexandre Courtiol}
\institute[IZW]{Leibniz Institute of Zoo and Wildlife Research}%
\titlegraphic{
\vspace{0cm}
\hspace{1cm}
\includegraphics[height=2cm]{../figures/izw_logo}
\hspace{0.7cm}
\includegraphics[height=3cm]{../figures/physalia}
\hspace{1.3cm}
\includegraphics[height=2cm]{../figures/FU}
\hspace{1cm}
}
\date[June 2018]{\small June 2018}%

\begin{document}

\setlength{\topsep}{1pt}%space between input and output
<<echo = FALSE, message = FALSE>>=
options(width = 120)
library(knitr)
library(BeginR)
opts_chunk$set("size" = "scriptsize",
               "fig.width" = 5,
               "fig.height" = 5,
               "out.width" = "0.35\\linewidth",
               "out.height" = "0.35\\linewidth",
               "fig.align" = "center")
@

\AtBeginSection[]{
  \begin{frame}
  \frametitle{Getting started with \R}
  \setcounter{tocdepth}{1}
  \tableofcontents[currentsection]
  \end{frame}
}

\AtBeginSubsection[]{
  \begin{frame}
  \frametitle{Getting started with \R}
  \setcounter{tocdepth}{2}
  \tableofcontents[currentsubsection]
  \end{frame}
}

% first slide of the doc
\maketitle

\section{Some basic tests}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. the usual correlation tests:
\vfill
Just the correlation coefficient:
<<>>=
cor(x = iris$Sepal.Length, y = iris$Sepal.Width, method = "pearson")
@
\vfill
\pause
Or the actual test:
<<>>=
cor.test(x = iris$Sepal.Length, y = iris$Sepal.Width, method = "pearson")
@
\vfill
\pause
Note: two other \texttt{method}s are available: ``\texttt{spearman}'' \& ```\texttt{kendall}''.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
Note: many (not all) tests allow for the use of both standard and formula-based syntax:
\vfill
E.g.
<<>>=
cor.test(formula = ~ Sepal.Length + Sepal.Width, data = iris)
@
\vfill
is synonymous to:
<<>>=
cor.test(x = iris$Sepal.Length, y = iris$Sepal.Width)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two unpaired groups:
\vfill
The \emph{t}-test (parametric):
<<>>=
t.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
       y = iris$Sepal.Length[iris$Species == "setosa"])
@
\vfill
\pause
The Mann-Whitney U test (non-parametric):
<<>>=
wilcox.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
            y = iris$Sepal.Length[iris$Species == "setosa"])
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two paired groups:
\vfill
The paired \emph{t}-test (parametric):
<<>>=
t.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
       y = iris$Petal.Length[iris$Species == "versicolor"], paired = TRUE)
@
\vfill
\pause
The Wilcox-signed-rank test (non-parametric):
<<>>=
wilcox.test(x = iris$Sepal.Length[iris$Species == "versicolor"],
            y = iris$Petal.Length[iris$Species == "versicolor"], paired = TRUE)
@
\vfill
\pause
Note: if you forget to specify that the data are \texttt{paired}, it won't run the right test!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing more than 2 unpaired groups:
\vfill
The Kurskal-Wallis test (non-parametric):
<<>>=
kruskal.test(formula = Petal.Length ~ Species, data = iris)
@
\vfill
\pause
The ``\emph{test for equal means in a one-way layout}'' (parametric):
<<>>=
oneway.test(formula = Petal.Length ~ Species, data = iris)
@
\vfill
\pause
Note: linear models allow for more sofisticated parametric alternatives (see later).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing more than 2 paired groups:
\vfill
The Quade test (non-parametric):
<<>>=
quade.test(y = as.matrix(iris[, c("Petal.Length", "Sepal.Length", "Petal.Width")]))
@
\vfill
\pause
The Friedman test (non-parametric):
<<>>=
friedman.test(y = as.matrix(iris[, c("Petal.Length", "Sepal.Length", "Petal.Width")]))
@
\vfill
\pause
Note: linear mixed-effects models allow for more sofisticated parametric alternatives.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing variances between groups:
\vfill
The F-test (parametric):
<<>>=
var.test(x = iris$Sepal.Length, y = iris$Petal.Length) ## max 2 groups, must be normaly distributed
@
\vfill
\pause
The Bartlett test (parametric):
<<>>=
bartlett.test(formula = Sepal.Length ~ Species, data = iris)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing variances between groups (continues):
\vfill
The Fligner test (non-parametric):
<<>>=
fligner.test(formula = Sepal.Length ~ Species, data = iris)
@
\vfill
\pause
Note: also \texttt{ansari.test()} and \texttt{mood.test()} for rank-based two-sample test for a difference in scale parameters.
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing 2 distributions:
\vfill
The Kolmogorov-Smirnov test (non-parametric):
<<>>=
ks.test(x = iris$Sepal.Length, y = iris$Petal.Length)
@
\vfill
\pause
The Shapiro-Wilk Normality test (non-parametric):
<<>>=
set.seed(2L)
shapiro.test(x = rnorm(100))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing a binomial outcome to a probability (exact):
\vfill
The exact binomial test:
<<>>=
binom.test(x = 8, n = 10, p = 0.5) ## 8 heads out of 10 coin throws -> is the coin biased?
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) for comparing two independent proportions:
\vfill
The ``\emph{test of equal proportions}'':
<<>>=
prop.test(x = cbind(success = c(8, 4), failure = c(2, 6))) ## 8 heads out of 10 for one coin, 4 out of 10 for the other, do they differ?
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{\r \ provides many statistical tests out of the box}
E.g. test(s) of independence:
\vfill
The Fisher exact test:
<<fisher>>=
## check WorldPhones before running the code!
fisher.test(WorldPhones, simulate.p.value = TRUE, B = 100) ## simulation needed as too large for exact test!
@
\vfill
\pause
The Chi-squared test for independence:
<<>>=
chisq.test(WorldPhones)
@
\vfill
\pause
Note: the McNemar test is also available when the same subjects are measured in two conditions (see \texttt{?mcnemar.test}).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Many more simple statistical tests are available in \R \ packages}
Some examples:
\begin{itemize}
\item \texttt{coin} provides permutation implementations of many tests.
\item \texttt{nsm3} provides tons of non-parametric tests.
\item \texttt{PMCMR} provides post-hoc tests for non-parametric tests.
\item \texttt{nortest} provides several tests for normality.
\end{itemize}
\vfill
Note: this list is only a very small subset!!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{A note before we continue}
\r's original primary goal was to perform statistical analyses. So among the many thousands of
packages many focus on statistical tools and by no means I will try to cover or even
summarise this diversity.\\
\vspace{1cm}
I have chosen to only illustrate some of the tools I know and that I have used to show you how to do some statistics in \r.\\
\vspace{1cm}
Since time is limited, I will not for example illustrate any Bayesian methods, nor machine learning methods, although some good packages exist for that too!
\end{frame}


\section{Principal Component Analysis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA without packages}
PCA is a traditional method for dimentionality reduction:
<<>>=
head(USArrests)  ## original coordinates
@
\vfill
\pause
<<>>=
pca_US <- prcomp(~ Murder + Assault + Rape, data = USArrests, scale. = TRUE) ## scaling is not the default (but should be)
@
\vfill
\pause
<<>>=
head(pca_US$x)  ## new coordinates
@
\vfill
\pause
<<>>=
pca_US$rotation ## coefficients applied to original coordinate (after z-scoring them) to obtain the new coordinates by linear combination
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA without packages}
The first axis alone captures more than 78\% of the total variation in the data:
<<>>=
summary(pca_US)
@
\vfill
\pause
<<>>=
biplot(x = pca_US)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA with \texttt{ade4}}
\texttt{ade4} is a package with several multivariate tools, including the PCA:
<<>>=
library(ade4)
pca_US_ade4 <- dudi.pca(df = USArrests[, -3], scale = TRUE, scannf = FALSE)

summary(pca_US_ade4)
@
\vfill
\pause
Let us add voting data to this dataset:
<<>>=
USArrests$Vote <- rep("Trump", times = 50)
Clinton_state <- c("California", "Colorado", "Connecticut", "Delaware", "Hawaii", "Illinois", "Maine", "Maryland", "Massachusetts",
                   "Minnesota", "Nevada", "New Hampshire", "New Jersey","New Mexico", "New York", "Oregon", "Rhode Island", "Vermont",
                   "Virginia", "Washington")
USArrests[Clinton_state, "Vote"] <- "Clinton"
USArrests$Vote <- factor(USArrests$Vote)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{PCA with \texttt{ade4}}
The package allows a different kind of plot that is quite interesting:
<<out.width="0.85\\linewidth", fig.width=12>>=
par(mfrow = c(1, 2))
s.corcircle(dfxy = pca_US_ade4$c1)
s.class(dfxy = pca_US_ade4$l1, fac = USArrests$Vote, col = c("blue", "red"))
s.label(dfxy = pca_US_ade4$l1, label = rownames(USArrests), add.plot = TRUE, clabel = 0.5)
@
\vfill
\pause
Note: using \texttt{co} \& \texttt{li} instead of \texttt{c1} \& \texttt{l1} would respect the relative contribution of each principal component according to the variance they capture.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example of application of PCA: a revision of tiger taxonomy}
\centering
\includegraphics[height = 0.2\textheight]{../figures/tiger_title}\\
\includegraphics[height = 0.7\textheight]{../figures/tiger}
\end{frame}



\section{Linear Models}
\subsection{introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What is a linear model?}

A statistical model represents, often in considerably idealized form, the data-generating process (\url{https://en.wikipedia.org/wiki/Statistical_model}).

\pause
\vfill
In a linear model, the data-generating process is assumed to be a linear function: it is constructed from a set of terms by multiplying each term by a constant (a model parameter) and adding the results.
\pause
\vfill
\r \ allows to fit efficiently and easily all main kinds of linear models:
\begin{itemize}
\item classical linear models (t-test, correlation, linear regression, ANOVA, ANCOVA): LM
\item generalized linear models (logistic regression, Poisson regression\dots): GLM
\item linear mixed-effects models: LMM
\item generalized linear mixed-effects models: GLMM
\item general additive models \& general additive mixed models: GAM \& GAMM
\end{itemize}
\vfill
\pause
Note: I have a 100 hours course on the topic (\url{https://github.com/courtiol/LM2GLMM}) but it may be a bit terse without the bla bla\dots
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Linear models in \R}
\r \ is very rich in terms of capabilities to fit linear models due to an increasing number of dedicated packages!
\vfill
For now, no other software seems to be remotely as good (prognostic: only \texttt{Julia} or \texttt{Python} may change that within a decade but I find it unlikely).
\vfill
\pause
\begin{center}
\begin{tabular}{|l|c|c|}%
\hline
Models & Packages for fitting & Helper packages\\
\hline
LM & none; \texttt{spaMM} & \texttt{car}; \texttt{lmtest}; \texttt{visreg}\\
GLM & none; \texttt{spaMM} & \texttt{car}; \texttt{DHARMa}; \texttt{visreg}\\
LMM & \texttt{lme4}; \texttt{spaMM}; \texttt{glmmTMB} & \texttt{DHARMa}; \texttt{pbkrtest}; \texttt{visreg}\\
GLMM & \texttt{lme4}; \texttt{spaMM}; \texttt{glmmTMB} & \texttt{DHARMa}; \texttt{pbkrtest}; \texttt{visreg}\\
GAM & \texttt{mgcv} & \texttt{DHARMa}; \texttt{visreg}\\
GAMM & \texttt{mgcv} &\\
\hline
\end{tabular}
\end{center}
\vfill
Note: those are my personal favorite ones, but they are plenty more out there.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Good books dealing with linear models in \R}
\begin{center}
\includegraphics[height=4cm]{../figures/Rbook}
\includegraphics[height=4cm]{../figures/PinheiroBates}
\includegraphics[height=4cm]{../figures/Gelman}
\includegraphics[height=4cm]{../figures/Zuur_MM}
\includegraphics[height=4cm]{../figures/Wood}
\end{center}
\vfill
Note: it is also useful to look at books focussed on statistics and not \r!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Preparing data for (G)LM(M) \& GA(M)}
To maximize the chances of success prepare your data as follow:
\begin{itemize}
\item one row = one observation (if repeated measures, use several rows!)
\item qualitative variables of class \texttt{factor} (check the levels, drop unused ones, set the reference properly)
\item no \texttt{NA} (models can somewhat deal with them but it is a major source of headackes)
\item data frames (i.e. object of class \texttt{data.frame}) and not tibbles (\texttt{tbl})
\end{itemize}
\vfill
\pause
Example of a good dataset:
<<>>=
head(iris)
str(iris)
any(is.na(iris))
@
\end{frame}


\subsection{traditional linear model (LM)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{LM: notation}
Simple notation:\\
\vspace{1cm}
$y_i = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_p} \times x_{p,i} + \varepsilon_i$
\vfill
\pause
$y_i = \hat{y_i} + \varepsilon_i$
\vfill
\pause
\begin{itemize}
\item $y_i$ = the observations to explain / response variable / dependent variable
\item $\hat{y_i}$ = the fitted values
\item $x_{j,i}$ = constants derived from the predictors / explanatory variables / independent variables
\item $\hat{\beta_j}$ = the (model parameter / regression coefficient) estimates
\item $\varepsilon_i$ = the residuals (i.e. the estimates for the error which is Gaussian with constant variance)
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{LM: notation}
Matrix notation:\\
\vspace{1cm}
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \dots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\hat{\beta_0} \\ \hat{\beta_1} \\ \hat{\beta_2} \\ \dots \\ \hat{\beta_p}
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \dots \\ \varepsilon_n
\end{bmatrix}
$$
\vfill
\pause
$Y = X \widehat{\beta} + \varepsilon = \widehat{Y} + \varepsilon$
\vfill
$\varepsilon = Y - \widehat{Y}$
\vfill
\pause
\begin{itemize}
\item $Y$ = the vector of observations
\item $Y$ = the vector of fitted values
\item $X$ = a matrix called the design matrix (or the model matrix)
\item $\varepsilon$ = the vector of residuals
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~ Petal.Width, data = iris)
@
\vfill
\pause
<<>>=
formula(mod)  ## the formula
@
\vfill
\pause
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
\pause
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
\pause
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species + Petal.Width:Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width*Species, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  Petal.Width/Species, data = iris)  ## dangerous
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  1, data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM: specifications}
\r \ formula notation:\\
<<>>=
mod <- lm(Petal.Length ~  ., data = iris)
@
\vfill
<<>>=
formula(mod)  ## the formula
@
\vfill
<<>>=
head(model_frame <- model.frame(mod))  ## the data used for the fit
@
\vfill
<<>>=
head(model.response(model_frame))  ## the response variable
@
\vfill
<<>>=
head(model.matrix(mod))  ## the model matrix
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Understanding the design matrix}
It is most important to understand the design matrix because the interpretation of the parameters depend on it!
\vfill
The design matrix depends on:
\begin{itemize}
\item your formula
\item your data
\item the contrasts (for qualitative variables)
\end{itemize}
\vfill
\pause
By default, each category is compared to the first one:
<<>>=
mod <- lm(Petal.Length ~  Species, data = iris)
model.matrix(mod)[c(1, 51, 101), ]
@
\vfill
\pause
But other several alternative exist; e.g.:
<<>>=
mod2 <- lm(Petal.Length ~  Species, data = iris, contrasts = list(Species = "contr.sum"))
model.matrix(mod2)[c(1, 51, 101), ]
@
\vfill
\pause
Note 1: default contrats (\texttt{"contr.treatment"}) are easy to interpret!
\vfill
\pause
Note 2: contrasts do not alter predicted values and thus likelihood, AIC\dots
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Understanding the design matrix}
Challenge: find out whether these different representations of gender are equivalent or not?
\vfill
\begin{itemize}
\item \texttt{"boy"} vs \texttt{"girl"}
\item \texttt{"male"} vs \texttt{"female"}
\item \texttt{0} vs \texttt{1}
\item \texttt{1} vs \texttt{2}
\item \texttt{TRUE} vs \texttt{FALSE}
\end{itemize}
\vfill
Note: no need to fit a model, use the function \texttt{model.matrix()} with a formula!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM outputs: parameter estimates}
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
Simply printing the object provides you with the parameter estimates:
<<>>=
mod
@
\vfill
\pause
If you need to work with them, use the specific extractor instead:
<<>>=
coefficients(mod) ## or coef(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM outputs: parameter estimates}
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
@
\vfill
You can also easily extract the covariance matrix of the estimates:
<<>>=
vcov(mod)
@
\vfill
\pause
And thus the standard errors:
<<>>=
sqrt(diag(vcov(mod)))
@
\vfill
\pause
You can also get confidence intervals:
<<>>=
confint(mod)
@
\vfill
\pause
Note: that reveals that there are much more information in the object \texttt{mod} than it is being printed!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM outputs: the model object}
The fitted model object is in fact a big list of class \texttt{"lm"}:
<<>>=
class(mod)
typeof(mod)
names(mod)
@
\vfill
So you can extract information from it; e.g.:
<<>>=
mod$df.residual
@
but it is safer to use extractors if they are available!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM: example of other outputs}
There are quite a few extractors out there:
<<>>=
logLik(mod)
AIC(mod)
@
\vfill
\pause
Here is how you can get the list of \texttt{S3} methods for the class \texttt{"lm"}:
<<>>=
methods(class = "lm")
@
\vfill
Note: the list will change depending on the packages that are attached to the \r \ session!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: coefficients}
For LM, simply use \texttt{summary()}:
<<>>=
summary(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: predictors}
Don't use the default \texttt{anova()} function which perform type-I analysis-of-variance:
<<>>=
anova(mod)
@
\vfill
Instead use the better function \texttt{Anova()} from the package \texttt{car} which perfroms type-II analysis-of-variance:
<<message = FALSE>>=
library(car)
Anova(mod)
@
\vfill
\pause
Note: p-values are the same no matter the order of the predictors in the formula for type-II (but not for type-I!).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM tests: the overall model}
Before looking at significance for estimates or predictor, always start by checking that your model fits the data better than a null model:
\vfill
<<>>=
mod <- lm(Petal.Length ~  Petal.Width + Species, data = iris)
mod_null <-  lm(Petal.Length ~  1, data = iris)
anova(mod, mod_null)
@
\vfill
Note 1: this was also given at the bottom of the summary table!\\
\vspace{1em}
Note 2: here using \texttt{anova()} is perfectly fine!
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: fitted values}
You can easily obtain the prediction for your observation (i.e. fitted values):
<<>>=
fitted(mod)[1:39]
@
\vfill
\pause
As expected, observations are equal to the fitted values + residuals:
<<>>=
head(cbind("response" = model.response(model.frame(mod)),
           "fitted" = fitted(mod),
           "resid" = residuals(mod),
           "fitted + resid" = fitted(mod) + residuals(mod)))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: fast and dirty plot}
<<"fig.width" = 10, "out.width" = "0.7\\linewidth">>=
library(visreg)
par(mfrow = c(1, 2))
visreg(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM predictions: fast \& less dirty plot}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
library(visreg)
library(ggplot2)
visreg(fit = mod, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE) +
  theme_classic()
@
\vfill
Note: if you have different quantitative predictors you can specify the value for the non focal predictor using the argument \texttt{"cond"}.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: by ``hand''}
The most difficult step is to create the data frame defining the predictor values:
<<message = FALSE>>=
library(dplyr)
data_for_predictions <- iris %>%
                            group_by(Species) %>%
                            do(data.frame(Petal.Width = seq(min(.$Petal.Width), max(.$Petal.Width), length.out = 30))) %>%
                            data.frame()
@
\begin{columns}
\column{0.4\linewidth}
<<>>=
head(data_for_predictions)
@
\column{0.4\linewidth}
<<>>=
tail(data_for_predictions)
@
\end{columns}
\vfill
\pause
Then, it is easy:
<<>>=
pred_mod <- predict(object = mod, newdata = data_for_predictions, interval = "confidence") ## prediction intervals are also possible!
head(pred_mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{LM predictions: by ``hand''}
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
data_for_plot <- cbind(pred_mod, data_for_predictions)
ggplot(data = data_for_plot, mapping = aes(x = Petal.Width, y = fit, colour = Species)) +
  geom_line() +
  geom_ribbon(mapping = aes(ymin = lwr, ymax = upr, fill = Species), alpha = 0.2) +
  geom_point(data = iris, mapping = aes(y = Petal.Length, x = Petal.Width, colour = Species)) +
  labs(x = "Petal Width", y = "Petal Length") +
  theme_classic()
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM assumptions: generalities}
Model structure:
\begin{itemize}
\item linearity
\item lack of perfect multicollinearity (design matrix of full rank)
\item predictor variables have fixed values
\end{itemize}
\pause
\vfill
Errors:
\begin{itemize}
\item independence (no serial autocorrelation)
\item constant variance (homoscedasticity)
\item normality
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: linearity}
Departure from linearity can originate from a multitude of reasons and can create all kinds of problems.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item thinking
\item other assumptions violated
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item different model structure $\rightarrow$ change the formula
\item transform one or several predictors (e.g. polynomials) $\rightarrow$ function \texttt{poly()}
\item transform the response (e.g. log and power transformation) $\rightarrow$ function \texttt{powerTransform()} in \texttt{car} (see later)
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item non-linear models $\rightarrow$ function \texttt{nls} or dedicated package (e.g. \texttt{nlme})
\item general additive models $\rightarrow$ package \texttt{mgcv}
\end{itemize}
\vfill
\pause
Quiz: can you express the following models as LM?
\begin{itemize}
\item $y_i = \hat{\alpha} + \varepsilon_i$
\item $y_i = x_i^{\hat{\beta}} + \varepsilon_i$
\item $y_i = \hat{\alpha} + \hat{\beta_1} x_i + \hat{\beta_2} x_i^2 + \hat{\beta_3} x_i^3 + \varepsilon_i$
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: lack of perfect multicollinearity}
The number of parameters to be estimated must be equal to the rank of the design matrix.
\vfill
Caused by having less data than parameters or when there is linear dependence between the column vectors of the design matrix. In such case, some parameters cannot be computed.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item plot the predictors against each other $\rightarrow$ function \texttt{pairs()}
\item \texttt{findLinearCombos} from the package \texttt{caret}
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item change design matrix (change parameterization or drop redundant effects) $\rightarrow$ argument \texttt{formula}
\item change the experimental design
\item collect more data
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item none
\end{itemize}
\vfill
\pause
Note: strong albeit imperfect collinearity is not great either; possible check correlation between estimates ($\rightarrow$ \texttt{cov2cor(vcov(mod))}) and variance inflation factors ($\rightarrow$ \texttt{vif(mod)}).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: predictor variables have fixed values}
The dependent variable are represented by fixed values.
\vfill
The presence of measurement errors is the main cause of violation. Violation can trigger both estimates and tests to be biased.
\vfill
\pause
Diagnostics:
\begin{itemize}
\item thinking \& replication
\end{itemize}
\vfill
\pause
Solutions:
\begin{itemize}
\item often ignored in practice
\item better measurements
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item multipurpose numerical approaches $\rightarrow$ function \texttt{optim()} or dedicated packages (e.g. \texttt{nloptr}, \texttt{rjags}, \texttt{nimble}, \texttt{rstan})
\item errors-in-variables models $\rightarrow$ not much directly but any procedure allowing for latent variables can handle that; packages (e.g. \texttt{sem}, \texttt{lavaan}, \texttt{OpenMX})
\item reduced major axis regression $\rightarrow$ dedicated packages (e.g. \texttt{lmodel2})
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: independence (no serial autocorrelation)}
A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). Lack of independence increases the risk of false positive (sometimes a lot).\\
\vspace{1em}
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 1)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: independence (no serial autocorrelation)}
A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). Lack of independence increases the risk of false positive (sometimes a lot).\\
\vspace{1em}
Diagnostic by Durbin-Watson test:
<<>>=
durbinWatsonTest(mod) ## from package car (DW varies between 0 & 4, 2 is best, you wish for non-significant p-value)
@
Note: the alternative from the package \texttt{lmtest} offer to rank the residuals according to a variable.
\vfill
\pause
Solutions:
\begin{itemize}
\item transformation or different model structure (see linearity)
\item aggregation or sub-sampling
\end{itemize}
\pause
\vfill
Alternatives:
\begin{itemize}
\item general additive models (GAM and GAMM) $\rightarrow$ dedicated package \texttt{mgcv}
\item mixed models (LMM and GLMM) $\rightarrow$ dedicated packages (e.g. \texttt{spaMM}, \texttt{lme4})
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: constant variance (homoscedasticity)}
Heteros(c/k)edasticity can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. It can create both false positives and false negative.
\vfill
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 3)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: constant variance (homoscedasticity)}
Heteros(c/k)edasticity can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. It can create both false positives and false negative.
\vfill
Diagnostic by Breusch-Pagan test:
<<message = FALSE>>=
library(lmtest)
bptest(mod)  ## BP = df is best, you wish for non-significant p-value
@
\vfill
\pause
Solutions: modeling the heteroscedasticity
<<message = FALSE>>=
library(spaMM)
mod_heter_spaMM <- fitme(Petal.Length ~  Petal.Width + Species,
                         resid.model = ~ Species,
                         data = iris)
AIC(mod)
print(AIC(mod_heter_spaMM)) ## much better fit!
@
\pause
\vfill
Alternatives:
\begin{itemize}
\item GLM (if stemming from an expected relationship between mean and variance) $\rightarrow$ function \texttt{glm}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: normality}
The distribution of residuals can be skewed, this is often caused by the presence of outliers, and/or when the process generating the data is very different from normal (e.g. Poisson, Binomial...).
\vfill
\pause
Diagnostic by eye:
<<>>=
plot(mod, which = 2)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: normality}
The distribution of residuals can be skewed, this is often caused by the presence of outliers, and/or when the process generating the data is very different from normal (e.g. Poisson, Binomial...).
\vfill
Diagnostic by test (many test are possible):
<<>>=
shapiro.test(mod$residuals)  ## stat = 1 when normal, you wish for non-significant p-value
@
\vfill
\pause
Solutions:
\begin{itemize}
\item transformation or different model structure (see linearity)
\item taking outliers out (mindfully!)
\end{itemize}
\vfill
\pause
Alternatives:
\begin{itemize}
\item GLM (if stemming from the data generating process) $\rightarrow$ function \texttt{glm}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: simple glimpse}
You can check all assumptions about the erros at once:
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.4\\linewidth", "out.height" = "0.4\\linewidth">>=
par(mfrow = c(2, 2))
plot(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: outliers}
There is a powerful function in \r:
<<>>=
influence.measures(mod)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{LM assumptions: outliers}
Interpretation of the output from \texttt{influence.measures(mod)}:
\vfill
\begin{small}
\begin{itemize}
\item<1-> \texttt{dfb.1\_} $\rightarrow$ extent to which the intercept changes if a given observation is dropped
\item<1-> \texttt{dfb.Pt.W} $\rightarrow$ extent to which the slope for \texttt{Petal.Width} changes if a given observation is dropped
\item<1-> \texttt{dfb.Spcsvrs} $\rightarrow$ extent to which the estimate for \texttt{versicolor} changes if a given observation is dropped
\item<1-> \texttt{dfb.Spcsvrg} $\rightarrow$ extent to which the estimate for \texttt{virginica} changes if a given observation is dropped
\item<2-> \texttt{dffit} $\rightarrow$ extent to which the predicted y-values changes if a given observation is dropped (scaled by the standard deviation of the fit at the point)
\item<3-> \texttt{cov.r} $\rightarrow$ extent to which the covariance matrix of parameter estimates changes if a given observation is dropped
\item<4-> \texttt{cook.d} $\rightarrow$ $F$ statistics comparing simultaneously the changes in all estimates when the observation is dropped or not
\item<5-> \texttt{hat} $\rightarrow$ diagonal element of the hat matrix (the hat values); extent to which an observation is unusual in terms of X values (leverage)
\item<6-> \texttt{inf} $\rightarrow$ some overal add hoc receipe to spot influential observation (not to be taken too seriously)
\end{itemize}
\end{small}
\vfill
Note: for the df-betas, the name would change for another model as they use the abbreviated name of the estimates:
<<>>=
abbreviate(stats:::variable.names.lm(mod))
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: outliers}
There are also plotting possibilitites:
<<"fig.width" = 12, "out.width" = "0.9\\linewidth">>=
par(mfrow = c(1, 3))
plot(mod, which = 4:6)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: simple glimpse at residuals}
What would it look like if it was perfect?
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.4\\linewidth", "out.height" = "0.4\\linewidth">>=
iris$Fake.Petal.Length <- simulate(object = mod)[, 1]  ## redo it, it will change each time!
mod_perfect <- lm(Fake.Petal.Length ~  Petal.Width + Species, data = iris)
par(mfrow = c(2, 2))
plot(mod_perfect)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Fixing attempt:
\vfill
<<"fig.width" = 7, "fig.height" = 7, "out.width" = "0.4\\linewidth", "out.height" = "0.4\\linewidth">>=
bc <- powerTransform(mod)
iris$Petal.Length_bc <- bcPower(iris$Petal.Length, lambda = bc$lambda)
mod_bc <- lm(Petal.Length_bc ~ Petal.Width + Species, data = iris)
par(mfrow = c(2, 2))
plot(mod_bc)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Plotting predictions:
\vfill
<<"fig.width" = 7, out.width = "0.5\\linewidth">>=
visreg(fit = mod_bc, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE) +
  theme_classic()
@
\vfill
Note: that is not very useful because it is on the BoxCoxed scale!
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t, fragile]{LM assumptions: fixing iris?}
Plotting predictions:
\vfill
<<"fig.width" = 7, "out.width" = "0.5\\linewidth">>=
visreg(fit = mod_bc, xvar = "Petal.Width", by = "Species", overlay = TRUE, gg = TRUE,
       trans = function(x) bcnPowerInverse(x, lambda = bc$lambda, gamma = 0), partial = TRUE) +
  theme_classic()
@
\end{frame}


\subsection{generalised linear models (GLM)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The generalised linear model: what for?}
GLM are used for fitting data generating processes for which a relationship between mean and variance is expected.
\vfill
\pause
That includes the analysis of:
\vspace{1em}
\begin{itemize}
\item binary events (probabilities)
\vspace{1em}
\item binomial events (probabilities)
\vspace{1em}
\item Poisson processes (counts)
\vspace{1em}
\item negative binomail processes (counts)
\vspace{1em}
\item variances (positive continuous)
\end{itemize}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]{The generalised linear model: specifications}
Definition:\\
\vspace{1em}
$\text{Y} = g^{-1}(\widehat{\eta})+ \varepsilon = g^{-1}(\text{X}\widehat{\beta}) + \varepsilon$
\vfill
with:
\begin{itemize}
\item $\hat{\eta_i} = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_{p}} \times x_{p,i}$
\item $\text{E}(\text{Y}) = \mu = g^{-1}(\eta)$
\item $\text{Var}(\text{Y}) = \phi\text{V}(\mu)$
\end{itemize}
\vfill
Notation:
\begin{itemize}
\item $\eta$ the linear predictor
\item $g$ the link function ($g^{-1}$ is sometimes called the mean function)
\item $\text{V}$ the variance function
\item $\phi$ is the dispersion parameter
\end{itemize}
\vfill
\pause
This is identical to the LM if:
\begin{itemize}
\item $\mu = g^{-1}(\eta) = \eta$, thus if $g$ is the identity function
\item $\phi = \sigma^2$, thus if the dispersion parameter equals the error variance
\item $\text{V}(\mu) = 1$, thus if the variance function is constant
\end{itemize}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The generalised linear model: the \texttt{Challenger} dataset}
\begin{center}
\includegraphics[height = 5cm]{../figures/Challenger}
\end{center}
<<>>=
head(Challenger, n = 3L)
@
\vfill
Note: we will study both the probability that one oring fails (binary event) or that at least one oring fails (binomial event) as a function of the temperature.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The generalised linear model: the \texttt{VonBort} dataset}
\begin{center}
\includegraphics[height = 5cm]{../figures/prussians}
\end{center}
<<>>=
head(VonBort, n = 3L)
@
\vfill
Note: we will compare the number of deaths caused by horse (or mule) kicks between the 14 corps of the Prussian army.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{The generalised linear model: specifications}
In \r \ notation:\\
\vspace{1em}
<<>>=
Challenger$issue <- Challenger$oring_dt > 0

mod_challenger_binar <- glm(issue ~ temp, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
\pause
<<>>=
Challenger$oring_ok <- Challenger$oring_tot - Challenger$oring_dt

mod_challenger_binom <- glm(cbind(oring_dt, oring_ok) ~ temp, family = binomial(link = "logit"), data = Challenger)
@
\vspace{1em}
\pause
<<>>=
mod_horsekick <- glm(deaths ~ corps, family = poisson(link = "log"), data = VonBort)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The generalised linear model: specifications}
The \texttt{family} object contains all kinds of useful information required for the fitting procedure:
<<>>=
names(binomial(link = "logit"))
@
\vfill
\pause
The link function:
<<>>=
probs <- seq(0.1, 0.9, by = 0.1)
logits <- binomial(link = "logit")$linkfun(mu = probs)
logits
@
\vfill
\pause
The inverse link function:
<<>>=
binomial(link = "logit")$linkinv(eta = logits)
@
\vfill
\pause
The variance function:
<<>>=
binomial(link = "logit")$variance(mu = probs)
@
\vfill
\pause
Note: you can use these functions to better understand GLM or when you need them to process some outputs.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The outputs are similar to those from \texttt{lm} fits!}
<<>>=
mod_challenger_binar
confint(mod_challenger_binar)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The outputs are similar to those from \texttt{lm} fits!}
<<>>=
mod_challenger_binom
confint(mod_challenger_binom)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The outputs are similar to those from \texttt{lm} fits!}
<<>>=
mod_horsekick
head(confint(mod_horsekick))
@
\vfill
\pause
Note: \alert{but the interpretation of the parameters is very different since they are expressed on the scale of the linear predictor!!}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{Interpreting estimates}
There is no general receipe, it all depends on the link function used\dots
\vfill
\pause
\begin{itemize}
\item For logistic regressions (\texttt{link = "logit"}; not for all binomial models), use odd-ratios:
\end{itemize}
<<>>=
exp(coef(mod_challenger_binar)["temp"])
1/exp(coef(mod_challenger_binar)["temp"])
@
Every decrease by one degree increases the odd of failure for at least one oring by \Sexpr{round(1/exp(coef(mod_challenger_binar)["temp"]), 1)} time!
\vfill
<<>>=
exp(coef(mod_challenger_binom)["temp"])
1/exp(coef(mod_challenger_binom)["temp"])
@
Every decrease by one degree increases the odd of failure for exactly one oring by \Sexpr{round(1/exp(coef(mod_challenger_binom)["temp"]), 1)} time!
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, t]{The generalised linear model: challenge}
Try to understand what influenced survival (i.e. access to lifeboats) during the Titanic disaster.
\begin{center}
\includegraphics[height = 5cm]{../figures/Titanic}
\end{center}
<<>>=
head(TitanicSurvival)
@
\end{frame}


\subsection{other linear models}

\end{document}
